# SDXL Temporal Adapter Video Inference Guide

This document explains how to generate high quality anime videos using your trained temporal adapter + SDXL base, running under your **SDXLAnime_BestHQScheduler**.

---

## ðŸ”¥ Goal

Produce **2â€“8s anime video** clips at 768â€“1024 resolution with:

* Smooth temporal consistency
* Minimal flicker
* Sharp SDXL-quality detail
* Stable motion across frames

---

# 1. Required Components

| Component        | Must be loaded                                |
| ---------------- | --------------------------------------------- |
| SDXL Base        | official base weights                         |
| VAE              | SDXL default or animeâ€‘optimized VAE           |
| Temporal Adapter | your trained .safetensors                     |
| Scheduler        | `SDXLAnime_BestHQScheduler` (DPM++ 2M Karras) |

Load SDXL normally â†’ then inject temporal UNet â†’ load temporal weights.

---

# 2. Recommended Generation Settings

| Setting    | Value                      |
| ---------- | -------------------------- |
| Steps      | 18â€“34 (sweet spot 24â€“28)   |
| CFG Scale  | **5.5â€“7.0** (6.25 ideal)   |
| Scheduler  | DPM++ 2M Karras            |
| Frames     | 12â€“24 for smooth clips     |
| Resolution | 768 or 1024                |
| Seed       | Fixed for consistent repro |

---

# 3. Noise Initialization (Critical)

Use **correlated noise** for stability:

```
base = randn(1,4,H/8,W/8)
latents = base.repeat(F,1,1,1) + 0.03*randn_like(latents)
```

This eliminates frameâ€‘toâ€‘frame identity drift.

---

# 4. Inference Loop Template

```
latents = init_latents(frames, height, width)
for t in scheduler.timesteps:
    pred = unet(latents, t, encoder_hidden_states=text_embeds, num_frames=frames).sample
    latents = scheduler.step(pred, t, latents).prev_sample
```

Then decode frames with VAE â†’ encode to mp4.

---

# 5. Video Assembly Command

```
ffmpeg -framerate 12 -i frame_%05d.png -c:v libx264 -crf 16 -pix_fmt yuv420p out.mp4
```

For smoother playback:

```
ffmpeg -i out.mp4 -vf minterpolate=fps=24 out_24fps.mp4
```

---

# 6. Advanced Quality Tricks

### 1) Motion Amplify

```
latents[t] = 0.9 * latents[t] + 0.1 * latents[t-1]
```

Smooths jitter in fast scenes.

### 2) Prompt Evolution

```
prompts = ["girl running","girl jumps","girl lands"]
```

Use perâ€‘segment conditioning for choreography.

### 3) Keyframe Anchoring

Reuse the same denoised latent every X frames to prevent drift.

```
if frame % 4 == 0:
    freeze_latent = latents.clone()
```

---

# 7. Expected Quality Levels

| Training Hours | Result                                        |
| -------------- | --------------------------------------------- |
| 8h             | Good â€” stable, usable video                   |
| 12h            | Nearâ€‘SOTA â€” crisp, fluid animation            |
| 20h+           | SOTA â€” minimal blur & longâ€‘sequence stability |

---

# Done ðŸŽ¬

You now have full video inference instructions.

If you'd like, I can generate:

* `inference_video.py` runnable script
* prompt scheduler for scene changes
* autoâ€‘upscaler + frame interpolation pipeline
