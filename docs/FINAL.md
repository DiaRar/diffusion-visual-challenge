# FINAL.md — SOTA SDXL Anime Video Pipeline (Contest-Compliant)

This document is the **single source of truth** for your entire anime video generation system under the **CS492C Visual Generation Contest constraints**.

> **CRITICAL RULE UPDATE (Slack Clarifications):**
> *   **SDXL 1.0** is the primary backbone. **SD 2 (Stable Diffusion 2)** is allowed as a lightweight alternative.
> *   **No External Visual Input at Inference**: You cannot use existing images, sketches, or manually drawn pose maps as input to ControlNet/img2img.
> *   **Internal Generation Allowed**: If your model generates a conditioning signal (e.g., text-to-image -> pose extraction), you CAN use that signal to guide further generation.
> *   **AnimateDiff Allowed**: Explicitly permitted as an "additional neural network for guidance".
> *   **Post-Processing Allowed**: Basic cropping, zooming, scaling of final output is permitted.
> *   **Data Restrictions**: Fine-tuning allowed only with FREE data (no paid assets).

---

# 1. System Goals

### **Primary Goal**

*   Produce **high‑quality anime images** (768×768 or 1024×1024)
*   SDXL (or SD2) + Anime LoRAs + LCM (4–6 steps)

### **Secondary Goal (Stretch)**

*   Produce **8–10 second 768p anime videos**
*   SDXL/SD2 + AnimateDiff Motion Module
*   **Self-generated** ControlNet guidance (no external images)
*   Minimal flicker, consistent identity, stable backgrounds

---

# 2. Architectural Components

## 2.1 Base Model (Required)

*   **Stable Diffusion XL Base 1.0** OR **Stable Diffusion 2**
*   Use: `torch.float16` or `bfloat16`
*   VAE: `sdxl-vae-fp16-fix` (for SDXL) or standard SD2 VAE

## 2.2 LoRAs (Anime Style)

### Required LoRAs
1.  **Pastel Anime XL** — Main style backbone (weight 0.7–0.85)
2.  **Anime Flat Color XL** — Flat-color, stable-cel look (weight 0.15–0.3)

### Optional LoRAs
*   **Character LoRA** — if identity stability needed (weight 0.6–0.8)
*   **Aesthetic Anime LoRA** — small polish (weight 0.1–0.2 max)

> Use **max 3 LoRAs** simultaneously to avoid interference.

---

# 3. ControlNets (Strict Compliance)

## ControlNet modules included:
*   **OpenPose** (pose stability, motion structure)
*   **Depth** (background + perspective stability)
*   **LineArt / Canny** (outline stabilization)

## CRITICAL: Conditioning Rules
**ALL ControlNet conditioning must be internally generated.**

**✅ Allowed:**
*   Text-to-Image generation -> Extract Pose/Depth -> Use as ControlNet input for Video.
*   Text-to-Image generation -> Extract Canny edges -> Use as ControlNet input.
*   Using AnimateDiff with text prompts only (no ControlNet).

**❌ NOT Allowed:**
*   Inputting an external image (e.g., from Google Images, ArtStation) to guide generation.
*   Inputting a hand-drawn sketch or stick figure.
*   Inputting a manually created/edited depth map or pose map.
*   Using commercial tools (ChatGPT, Midjourney) to generate the input.

---

# 4. AnimateDiff Temporal Module

### Required
*   **AnimateDiff Motion Module** (SDXL or SD2 version)

### Parameters
*   `num_frames=16` per segment
*   FPS: **8–12 fps**
*   Chaining: pass last-frame latent → next segment

### Optional
*   **Motion LoRA** (pan, zoom, drift) - limit to one at a time.

---

# 5. LCM Acceleration (Latent Consistency Model)

*   Load LCM LoRA for SDXL/SD2
*   Steps: **4–6**
*   CFG: **1.5–2.0**
*   Scheduler: **LCMScheduler** (automatically applied when LCM LoRA is enabled)
*   **Benefit**: 10×–20× speedup while maintaining quality

> **Note**: The system uses automatic scheduler selection:
> - **High-Quality Mode**: DPM++ 2M with Karras sigmas (20-30 steps, CFG 6.0)
> - **Fast Mode**: LCMScheduler with LCM LoRA (4-6 steps, CFG 1.7).

---

# 6. Inference Optimizations

*   Use SDPA / Flash Attention (PyTorch 2.x)
*   Enable `torch.compile(unet, mode="max-autotune")`
*   fp16 / bf16 everywhere
*   Disable safety checker
*   Use deterministic seeding
*   **Automatic Scheduler Selection**: No manual scheduler configuration needed
    * High-quality mode uses DPM++ 2M with Karras sigmas
    * Fast mode (LCM) automatically switches to LCMScheduler

---

# 7. Prompting Strategy

### Positive Prompt Template
```
(masterpiece, anime, clean cel-shading), detailed eyes, vivid colors,
<lora:pastelAnimeXL:0.8>, <lora:animeFlatColorXL:0.25>,
1 girl, flowing hair, dynamic pose, dramatic lighting
```

### Negative Prompt Template
```
bad anatomy, extra limbs, two heads, blurry, low detail, watermark,
text, distorted face, unstable shading
```

---

# 8. Full Pipeline Procedure (Compliant Video)

## **Step 1 — (Optional) Generate Internal Reference**
*   Use Text-to-Image (SDXL/SD2) to generate a starting frame or keyframe.
*   **Rule**: This image MUST be generated by your model, not imported.
*   Scheduler is automatically selected (DPM++ 2M for HQ, LCMScheduler if LCM enabled).

## **Step 2 — (Optional) Extract Control Signal**
*   If using ControlNet: Run OpenPose/Depth preprocessor on the *generated* image from Step 1.
*   Use this internal map to guide the video generation.

## **Step 3 — Initialize AnimateDiff**
*   Load Motion Model
*   Apply LoRAs + LCM
*   Set num_frames=16

## **Step 4 — Generate Segment**
*   Generate 16 frames.
*   Use last latent as context for next segment (smooth transitions).
*   Same automatic scheduler selection as image generation.

## **Step 5 — Post-Process**
*   Save frames → assemble to MP4 (ffmpeg).
*   Basic cropping/scaling allowed.
*   Frame interpolation (e.g., RIFE) allowed if open-source model.

---

# 9. Submission Requirements

### Deliverables
*   **Poster**: PDF, max 10MB.
*   **Source Code & Data**: Must reproduce results.
*   **Write-up**: PDF, max 4 pages. Must include:
    *   Project Title, Names, IDs
    *   Technical & Artistic description
    *   Reproduction steps
    *   **Citations**: List ALL pretrained checkpoints (SDXL, LoRAs, ControlNets, AnimateDiff) with links.

### Presentation
*   Online via Gather.town (Dec 8 or Dec 10).
*   Mandatory attendance.

---

# 10. Definition of Done

*   Reproducible SDXL/SD2 pipeline.
*   1–3 high-quality anime stills.
*   8–10 second stable anime video.
*   **Full compliance with contest rules (No external inputs).**
